config_file=./BERT/data/multi_cased_L-12_H-768_A-12/bert_config.json
init_checkpoint=multi_cased_L-12_H-768_A-12/bert_model.ckpt
vocab_file=./BERT/data/multi_cased_L-12_H-768_A-12/vocab.txt
label_id=./BERT/data/lazada_multilingual/label_dict.json
max_length=128
train_file=lcqmc/albert_tiny/train_tfrecords
dev_file=lcqmc/albert_tiny/test_tfrecords
model_output=lcqmc/albert_tiny/model/estimator/albert_tiny_two_stage_dropout_13
epoch=8
num_classes=2
train_size=238766
eval_size=12500
batch_size=64
model_type=bert
if_shard=2
is_debug=1
run_type=sess
opt_type="all_reduce"
num_gpus=4
parse_type=parse_batch
rule_model=normal
profiler="no"
train_op=adam
running_type=train
cross_tower_ops_type=paisoar
distribution_strategy=MirroredStrategy
load_pretrained=yes
warmup=warmup
decay=decay
with_target=""
input_target=""
distillation="normal"
temperature=2.0
distillation_ratio=1.0
num_hidden_layers=12
task_type=single_sentence_classification
classifier=order_classifier
mode="distillation"
multi_task_type="teacher,student"
multi_task_config="./BERT/t2t_bert/distributed_distillation/knolwedge_distillation_config.json"
distillation_config="./BERT/t2t_bert/distributed_distillation/distillation_config.json"
init_lr=1e-4