config_file=./BERT/data/gpt/gpt_config.json
init_checkpoint=product_title_lm/gpt/model/estimator/gpt_lm_product_title_sample_0824/model.ckpt-440000
vocab_file=./BERT/data/gpt/vocab.txt
label_id=./BERT/data/lcqmc/label_dict.json
max_length=64
train_file=abuse/data/train_tfrecords
dev_file=abuse/data/dev_tfrecords
model_output=product_title_lm/gpt/model/estimator/gpt_lm_product_title_sample_0824/
export_dir=product_title_lm/gpt/model/estimator/gpt_lm_product_title_sample_0824/export
epoch=5
num_classes=2
train_size=500000
eval_size=190190
batch_size=24
model_type=gpt
if_shard=2
is_debug=1
run_type=sess
opt_type="all_reduce"
num_gpus=1
parse_type=parse_batch
rule_model=normal
profiler="no"
train_op=adam_weight_decay_exclude
running_type=eval
cross_tower_ops_type=paisoar
distribution_strategy=MirroredStrategy
load_pretrained=yes
warmup=warmup
decay=decay
with_target=""
input_target=""
distillation="normal"
temperature=2.0
distillation_ratio=1.0
num_hidden_layers=12
mode="single_task"
task_type=gpt_pretrain