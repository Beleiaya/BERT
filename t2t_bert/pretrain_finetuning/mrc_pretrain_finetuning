config_file=./BERT/data/chinese_L-12_H-768_A-12/bert_config.json
init_checkpoint=chinese-bert_chinese_wwm_L-12_H-768_A-12/bert_model.ckpt
vocab_file=./BERT/data/chinese_L-12_H-768_A-12/vocab.txt
label_id=./BERT/data/lcqmc/label_dict.json
max_length=384
train_file=mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_5.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_6.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_7.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_8.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_9.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_10.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_11.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_12.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_13.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_14.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_15.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_16.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_17.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_18.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_19.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_0.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_1.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_2.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_3.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_4.tfrecords
dev_file=mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_0.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_1.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_2.tfrecords,mrc_search/mrc_search/pretrain_jieba/pretrain_jieba/chunk_3.tfrecords,mrc_search/mrc_search/pretrain_jieba/chunk_4.tfrecords
model_output=mrc_search/mrc_search/pretrain_jieba/bert_wwm_mrc_search_pretrain_1120_5_0717/
epoch=5
num_classes=2
train_size=5000000
eval_size=8802
batch_size=12
model_type=bert
if_shard=2
is_debug=1
run_type=estimator
opt_type="all_reduce"
num_gpus=1
parse_type=parse_batch
rule_model=normal
profiler="no"
train_op=adam_weight_decay_exclude
running_type=train
cross_tower_ops_type=paisoar
distribution_strategy=MirroredStrategy
load_pretrained=yes
warmup=warmup
decay=decay
with_target=""
input_target=""
distillation="normal"
temperature=2.0
distillation_ratio=1.0
num_hidden_layers=12
task_type=bert_pretrain
classifier=order_classifier
max_predictions_per_seq=5