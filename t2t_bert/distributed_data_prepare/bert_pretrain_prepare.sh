python ./t2t_bert/data_generator/my_create_pretrain_data.py \
	--buckets /data/xuht \
	--input_file sentence_embedding/new_data/cluster_corpus_fasttext_train.txt \
	--output_file sentence_embedding/new_data/cluster_corpus_fasttext_eval.txt \
	--vocab_file w2v/tencent_ai_lab/char_id.txt \
	--word_piece_model /data/xuht/sentence_embedding/cluster_corpus_label_dict.json \
	--do_lower_case True \
	--max_length 384 \
	--max_predictions_per_seq 20 \
	--dupe_factor 2 \
	--tokenizer_type 'word_piece'
