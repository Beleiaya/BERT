mpirun -np 2 \
 -H localhost:2 \
 python ./t2t_bert/distributed_bin/hvd_train_eval_api.py \
 --buckets "/data/xuht" \
 --config_file "./data/textcnn/textcnn.json" \
 --init_checkpoint "" \
 --vocab_file "porn/clean_data/textcnn/distillation/char_id.txt" \
 --label_id "porn/label_dict.json" \
 --max_length 128 \
 --train_file "porn/clean_data/textcnn/distillation/train_tfrecords" \
 --dev_file "porn/clean_data/textcnn/distillation/dev_tfrecords" \
 --model_output "porn/clean_data/textcnn/model/estimator/distillation/all_reduce_4_adam_weight_0228/" \
 --epoch 20 \
 --num_classes 5 \
 --train_size 1190267 \
 --eval_size 238054 \
 --batch_size 24 \
 --model_type "textcnn_distillation" \
 --if_shard 1 \
 --is_debug 1 \
 --run_type "sess" \
 --opt_type "hvd" \
 --num_gpus 4 \
 --parse_type "parse_batch" \
 --rule_model "normal" \
 --profiler "no" \
 --train_op "adam" \
 --running_type "train" \
 --cross_tower_ops_type "paisoar" \
 --distribution_strategy "MirroredStrategy" \
 --load_pretrained "no" \
 --w2v_path "w2v/tencent_ai_lab/char_w2v.txt" \
 --with_char "no_char" \
 --input_target "a" \
 --decay "no" \
 --warmup "no" \
 --distillation "distillation" \
 --temperature 2.0 \
 --distillation_ratio 1.0


